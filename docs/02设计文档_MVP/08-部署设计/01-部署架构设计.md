# AIåŠ©è¯„ç³»ç»ŸMVPéƒ¨ç½²æ¶æ„è®¾è®¡

---

**æ–‡æ¡£ç¼–å·ï¼š** HKHR-MVP-DEPLOY-001
**é¡¹ç›®åç§°ï¼š** AIåŠ©åŠ›æ•™å­¦è¯„ä»·åº”ç”¨ï¼ˆMVPéƒ¨ç½²æ¶æ„ï¼‰
**ç¼–åˆ¶å•ä½ï¼š** æ•™åŠ¡å¤„ä¿¡æ¯æŠ€æœ¯ä¸­å¿ƒ
**ç¼–åˆ¶æ—¥æœŸï¼š** 2025å¹´11æœˆ23æ—¥
**æ–‡æ¡£ç‰ˆæœ¬ï¼š** V1.0
**å®¡é˜…äººï¼š** é¡¹ç›®æŠ€æœ¯ç»„
**æ‰¹å‡†äººï¼š** é¡¹ç›®é¢†å¯¼å°ç»„

---

## æ–‡æ¡£ä¿®è®¢è®°å½•

| ç‰ˆæœ¬ | ä¿®è®¢æ—¥æœŸ | ä¿®è®¢å†…å®¹ | ä¿®è®¢äºº |
|------|----------|----------|--------|
| V1.0 | 2025-11-23 | åˆå§‹ç‰ˆæœ¬åˆ›å»º | é¡¹ç›®ç»„ |

---

## ç›®å½•

1. [å¼•è¨€](#1-å¼•è¨€)
   1.1 [æ–‡æ¡£ç›®çš„](#11-æ–‡æ¡£ç›®çš„)
   1.2 [éƒ¨ç½²ç›®æ ‡](#12-éƒ¨ç½²ç›®æ ‡)
   1.3 [éƒ¨ç½²åŸåˆ™](#13-éƒ¨ç½²åŸåˆ™)

2. [éƒ¨ç½²æ¶æ„æ€»è§ˆ](#2-éƒ¨ç½²æ¶æ„æ€»è§ˆ)
   2.1 [æ•´ä½“æ¶æ„å›¾](#21-æ•´ä½“æ¶æ„å›¾)
   2.2 [ç½‘ç»œæ¶æ„](#22-ç½‘ç»œæ¶æ„)
   2.3 [ç¯å¢ƒè§„åˆ’](#23-ç¯å¢ƒè§„åˆ’)
   2.4 [æŠ€æœ¯é€‰å‹](#24-æŠ€æœ¯é€‰å‹)

3. [å®¹å™¨åŒ–éƒ¨ç½²](#3-å®¹å™¨åŒ–éƒ¨ç½²)
   3.1 [Dockeré•œåƒè®¾è®¡](#31-dockeré•œåƒè®¾è®¡)
   3.2 [å®¹å™¨ç¼–æ’](#32-å®¹å™¨ç¼–æ’)
   3.3 [æœåŠ¡å‘ç°](#33-æœåŠ¡å‘ç°)
   3.4 [è´Ÿè½½å‡è¡¡](#34-è´Ÿè½½å‡è¡¡)

4. [æ•°æ®åº“éƒ¨ç½²](#4-æ•°æ®åº“éƒ¨ç½²)
   4.1 [PostgreSQLéƒ¨ç½²](#41-postgresqléƒ¨ç½²)
   4.2 [ç¼“å­˜éƒ¨ç½²](#42-ç¼“å­˜éƒ¨ç½²)
   4.3 [æ–‡ä»¶å­˜å‚¨éƒ¨ç½²](#43-æ–‡ä»¶å­˜å‚¨éƒ¨ç½²)
   4.4 [æ•°æ®å¤‡ä»½ç­–ç•¥](#44-æ•°æ®å¤‡ä»½ç­–ç•¥)

5. [CI/CDæµæ°´çº¿](#5-cicdæµæ°´çº¿)
   5.1 [æŒç»­é›†æˆ](#51-æŒç»­é›†æˆ)
   5.2 [æŒç»­éƒ¨ç½²](#52-æŒç»­éƒ¨ç½²)
   5.3 [è‡ªåŠ¨åŒ–æµ‹è¯•](#53-è‡ªåŠ¨åŒ–æµ‹è¯•)
   5.4 [å›æ»šæœºåˆ¶](#54-å›æ»šæœºåˆ¶)

6. [ç›‘æ§è¿ç»´](#6-ç›‘æ§è¿ç»´)
   6.1 [ç³»ç»Ÿç›‘æ§](#61-ç³»ç»Ÿç›‘æ§)
   6.2 [æ—¥å¿—ç®¡ç†](#62-æ—¥å¿—ç®¡ç†)
   6.3 [å‘Šè­¦æœºåˆ¶](#63-å‘Šè­¦æœºåˆ¶)
   6.4 [è¿ç»´è‡ªåŠ¨åŒ–](#64-è¿ç»´è‡ªåŠ¨åŒ–)

---

## 1. å¼•è¨€

### 1.1 æ–‡æ¡£ç›®çš„

æœ¬æ–‡æ¡£æè¿°äº†AIåŠ©è¯„ç³»ç»ŸMVPç‰ˆæœ¬çš„éƒ¨ç½²æ¶æ„è®¾è®¡ï¼ŒåŒ…æ‹¬æ•´ä½“æ¶æ„ã€ç½‘ç»œè®¾è®¡ã€å®¹å™¨åŒ–éƒ¨ç½²ã€CI/CDæµæ°´çº¿ã€ç›‘æ§è¿ç»´ç­‰å†…å®¹ï¼Œä¸ºç³»ç»Ÿéƒ¨ç½²å’Œè¿ç»´æä¾›è¯¦ç»†çš„æŠ€æœ¯æŒ‡å¯¼ã€‚

### 1.2 éƒ¨ç½²ç›®æ ‡

#### 1.2.1 æŠ€æœ¯ç›®æ ‡
- **é«˜å¯ç”¨æ€§**ï¼šç³»ç»Ÿå¯ç”¨æ€§ â‰¥ 99%
- **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•å’Œå¼¹æ€§ä¼¸ç¼©
- **å®‰å…¨æ€§**ï¼šæ»¡è¶³ä¿¡æ¯å®‰å…¨ç®¡ç†è¦æ±‚
- **æ€§èƒ½è¦æ±‚**ï¼šæ»¡è¶³æ€§èƒ½éœ€æ±‚è§„æ ¼

#### 1.2.2 è¿ç»´ç›®æ ‡
- **è‡ªåŠ¨åŒ–éƒ¨ç½²**ï¼šå®ç°è‡ªåŠ¨åŒ–éƒ¨ç½²å’Œå‘å¸ƒ
- **ç®€åŒ–è¿ç»´**ï¼šé™ä½è¿ç»´å¤æ‚åº¦å’Œæˆæœ¬
- **å¿«é€Ÿæ¢å¤**ï¼šæ•…éšœæ¢å¤æ—¶é—´ â‰¤ 30åˆ†é’Ÿ
- **æŒç»­ç›‘æ§**ï¼šå…¨æ–¹ä½çš„ç³»ç»Ÿç›‘æ§

### 1.3 éƒ¨ç½²åŸåˆ™

#### 1.3.1 åŸºç¡€è®¾æ–½å³ä»£ç 
- **ç¯å¢ƒæ ‡å‡†åŒ–**ï¼šæ‰€æœ‰ç¯å¢ƒä½¿ç”¨ä»£ç å®šä¹‰å’Œç®¡ç†
- **ç‰ˆæœ¬æ§åˆ¶**ï¼šåŸºç¡€è®¾æ–½å˜æ›´çº³å…¥ç‰ˆæœ¬æ§åˆ¶
- **è‡ªåŠ¨åŒ–ç®¡ç†**ï¼šè‡ªåŠ¨åŒ–ç¯å¢ƒé…ç½®å’Œç®¡ç†

#### 1.3.2 å®¹å™¨åŒ–éƒ¨ç½²
- **æ ‡å‡†åŒ–éƒ¨ç½²**ï¼šä½¿ç”¨å®¹å™¨æ ‡å‡†åŒ–åº”ç”¨éƒ¨ç½²
- **ç¯å¢ƒä¸€è‡´æ€§**ï¼šç¡®ä¿å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç¯å¢ƒä¸€è‡´
- **å¿«é€Ÿéƒ¨ç½²**ï¼šæ”¯æŒå¿«é€Ÿåº”ç”¨éƒ¨ç½²å’Œæ›´æ–°

#### 1.3.3 å¾®æœåŠ¡æ¶æ„
- **æœåŠ¡æ‹†åˆ†**ï¼šæŒ‰ä¸šåŠ¡åŠŸèƒ½æ‹†åˆ†æœåŠ¡
- **ç‹¬ç«‹éƒ¨ç½²**ï¼šæœåŠ¡å¯ç‹¬ç«‹éƒ¨ç½²å’Œæ‰©å±•
- **æ•…éšœéš”ç¦»**ï¼šå•ä¸ªæœåŠ¡æ•…éšœä¸å½±å“æ•´ä½“ç³»ç»Ÿ

---

## 2. éƒ¨ç½²æ¶æ„æ€»è§ˆ

### 2.1 æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "å¤–éƒ¨è®¿é—®å±‚"
        Users[ç”¨æˆ·]
        Internet[äº’è”ç½‘]
    end

    subgraph "CDNå±‚"
        CDN[CDNåŠ é€Ÿ]
    end

    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[è´Ÿè½½å‡è¡¡å™¨]
        WAF[Webåº”ç”¨é˜²ç«å¢™]
    end

    subgraph "åº”ç”¨å±‚"
        subgraph "å‰ç«¯é›†ç¾¤"
            FE1[å‰ç«¯å®¹å™¨1]
            FE2[å‰ç«¯å®¹å™¨2]
        end

        subgraph "åç«¯é›†ç¾¤"
            BE1[åç«¯å®¹å™¨1]
            BE2[åç«¯å®¹å™¨2]
            BE3[åç«¯å®¹å™¨3]
        end
    end

    subgraph "æ•°æ®å±‚"
        subgraph "æ•°æ®åº“é›†ç¾¤"
            DB1[(PostgreSQLä¸»åº“)]
            DB2[(PostgreSQLä»åº“1)]
            DB3[(PostgreSQLä»åº“2)]
        end

        subgraph "ç¼“å­˜é›†ç¾¤"
            Cache1[(RedisèŠ‚ç‚¹1)]
            Cache2[(RedisèŠ‚ç‚¹2)]
            Cache3[(RedisèŠ‚ç‚¹3)]
        end

        subgraph "æ–‡ä»¶å­˜å‚¨"
            Storage[å¯¹è±¡å­˜å‚¨]
        end
    end

    subgraph "ç›‘æ§å±‚"
        Monitor[ç›‘æ§ç³»ç»Ÿ]
        Logs[æ—¥å¿—ç³»ç»Ÿ]
        Alert[å‘Šè­¦ç³»ç»Ÿ]
    end

    subgraph "å¤–éƒ¨ç³»ç»Ÿ"
        Zhijiaoyun[èŒæ•™äº‘ç³»ç»Ÿ]
    end

    Users --> Internet
    Internet --> CDN
    CDN --> LB
    LB --> WAF
    WAF --> FE1
    WAF --> FE2
    WAF --> BE1
    WAF --> BE2
    WAF --> BE3

    BE1 --> DB1
    BE2 --> DB1
    BE3 --> DB1

    DB1 --> DB2
    DB1 --> DB3

    BE1 --> Cache1
    BE2 --> Cache2
    BE3 --> Cache3

    BE1 --> Storage
    BE2 --> Storage
    BE3 --> Storage

    BE1 --> Monitor
    BE2 --> Monitor
    BE3 --> Monitor
    DB1 --> Monitor
    Cache1 --> Monitor

    BE1 --> Logs
    BE2 --> Logs
    BE3 --> Logs

    Monitor --> Alert
    Logs --> Alert

    BE1 --> Zhijiaoyun
```

### 2.2 ç½‘ç»œæ¶æ„

#### 2.2.1 ç½‘ç»œæ‹“æ‰‘

```mermaid
graph TB
    subgraph "DMZåŒºåŸŸ"
        Internet[äº’è”ç½‘]
        Firewall[é˜²ç«å¢™]
        LB[è´Ÿè½½å‡è¡¡å™¨]
        WAF[Webé˜²ç«å¢™]
    end

    subgraph "åº”ç”¨ç½‘ç»œ"
        subgraph "WebæœåŠ¡å­ç½‘"
            FE[å‰ç«¯æœåŠ¡]
        end

        subgraph "APIæœåŠ¡å­ç½‘"
            BE[åç«¯æœåŠ¡]
        end
    end

    subgraph "æ•°æ®ç½‘ç»œ"
        subgraph "æ•°æ®åº“å­ç½‘"
            DB[(PostgreSQL)]
        end

        subgraph "ç¼“å­˜å­ç½‘"
            Cache[(Redis)]
        end
    end

    subgraph "ç®¡ç†ç½‘ç»œ"
        Monitor[ç›‘æ§æœåŠ¡]
        Backup[å¤‡ä»½æœåŠ¡]
    end

    Internet --> Firewall
    Firewall --> LB
    LB --> WAF
    WAF --> FE
    WAF --> BE

    BE --> DB
    BE --> Cache

    Monitor --> DB
    Monitor --> Cache
    Monitor --> BE
```

#### 2.2.2 å®‰å…¨ç»„è§„åˆ™

| å®‰å…¨ç»„ | åè®® | ç«¯å£èŒƒå›´ | æºåœ°å€ | æè¿° |
|--------|------|----------|--------|------|
| web-sg | HTTP | 80 | 0.0.0.0/0 | Webè®¿é—® |
| web-sg | HTTPS | 443 | 0.0.0.0/0 | å®‰å…¨Webè®¿é—® |
| app-sg | HTTP | 3000 | web-sg | åº”ç”¨æœåŠ¡ |
| db-sg | PostgreSQL | 5432 | app-sg | æ•°æ®åº“è®¿é—® |
| cache-sg | Redis | 6379 | app-sg | ç¼“å­˜è®¿é—® |
| monitor-sg | SSH | 22 | ç®¡ç†å‘˜IP | SSHè®¿é—® |

### 2.3 ç¯å¢ƒè§„åˆ’

#### 2.3.1 ç¯å¢ƒåˆ†ç±»

| ç¯å¢ƒç±»å‹ | ç”¨é€” | æœåŠ¡å™¨æ•°é‡ | é…ç½®è¦æ±‚ | éƒ¨ç½²æ–¹å¼ |
|----------|------|------------|----------|----------|
| å¼€å‘ç¯å¢ƒ | å¼€å‘è°ƒè¯• | 2å° | 2æ ¸4G | æ‰‹å·¥éƒ¨ç½² |
| æµ‹è¯•ç¯å¢ƒ | åŠŸèƒ½æµ‹è¯• | 3å° | 4æ ¸8G | è‡ªåŠ¨éƒ¨ç½² |
| é¢„ç”Ÿäº§ç¯å¢ƒ | éªŒæ”¶æµ‹è¯• | 3å° | 8æ ¸16G | è‡ªåŠ¨éƒ¨ç½² |
| ç”Ÿäº§ç¯å¢ƒ | æ­£å¼è¿è¡Œ | 5å° | 8æ ¸16G | è‡ªåŠ¨éƒ¨ç½² |

#### 2.3.2 ç¯å¢ƒéš”ç¦»ç­–ç•¥

```typescript
// ç¯å¢ƒéš”ç¦»é…ç½®
interface EnvironmentIsolation {
  network: {
    vpcId: string;
    subnets: {
      development: '10.0.1.0/24';
      testing: '10.0.2.0/24';
      staging: '10.0.3.0/24';
      production: '10.0.4.0/24';
    };
  };

  database: {
    development: 'ai_evaluation_dev';
    testing: 'ai_evaluation_test';
    staging: 'ai_evaluation_staging';
    production: 'ai_evaluation_prod';
  };

  domains: {
    development: 'dev.ai-evaluation.local';
    testing: 'test.ai-evaluation.local';
    staging: 'staging.ai-evaluation.com';
    production: 'app.ai-evaluation.com';
  };
}
```

### 2.4 æŠ€æœ¯é€‰å‹

#### 2.4.1 å®¹å™¨åŒ–æŠ€æœ¯

| æŠ€æœ¯ç»„ä»¶ | é€‰å‹ | ç‰ˆæœ¬ | é€‰æ‹©ç†ç”± |
|----------|------|------|----------|
| å®¹å™¨è¿è¡Œæ—¶ | Docker | 24.0 | ç”Ÿæ€æˆç†Ÿï¼Œå¹¿æ³›ä½¿ç”¨ |
| å®¹å™¨ç¼–æ’ | Docker Compose | 2.20 | ç®€å•æ˜“ç”¨ï¼Œé€‚åˆå°è§„æ¨¡éƒ¨ç½² |
| é•œåƒä»“åº“ | Docker Registry | 2.8 | å®˜æ–¹é•œåƒä»“åº“ |
| å®¹å™¨ç½‘ç»œ | Docker Network | å†…ç½® | å®¹å™¨é—´ç½‘ç»œé€šä¿¡ |

#### 2.4.2 åŸºç¡€è®¾æ–½æŠ€æœ¯

| æŠ€æœ¯ç»„ä»¶ | é€‰å‹ | ç‰ˆæœ¬ | é€‰æ‹©ç†ç”± |
|----------|------|------|----------|
| åå‘ä»£ç† | Nginx | 1.25 | é«˜æ€§èƒ½ï¼Œç¨³å®šå¯é  |
| æ•°æ®åº“ | PostgreSQL | 14 | åŠŸèƒ½å¼ºå¤§ï¼ŒACIDæ”¯æŒ |
| ç¼“å­˜ | Redis | 7.2 | é«˜æ€§èƒ½å†…å­˜æ•°æ®åº“ |
| ç›‘æ§ | Prometheus + Grafana | æœ€æ–° | å¼€æºç›‘æ§è§£å†³æ–¹æ¡ˆ |
| æ—¥å¿— | ELK Stack | 8.10 | å®Œæ•´çš„æ—¥å¿—åˆ†ææ–¹æ¡ˆ |

---

## 3. å®¹å™¨åŒ–éƒ¨ç½²

### 3.1 Dockeré•œåƒè®¾è®¡

#### 3.1.1 å‰ç«¯é•œåƒ

```dockerfile
# frontend/Dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

COPY . .
RUN npm run build

FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
COPY nginx.conf /etc/nginx/nginx.conf

EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

#### 3.1.2 åç«¯é•œåƒ

```dockerfile
# backend/Dockerfile
FROM denoland/deno:1.40.0-alpine

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY deno.json deno.lock ./
RUN deno cache --lock=deno.lock

# å¤åˆ¶æºç 
COPY src ./src

# åˆ›å»ºérootç”¨æˆ·
RUN addgroup -g 1001 -S deno
RUN adduser -S deno -u 1001
USER deno

EXPOSE 3000

CMD ["run", "--allow-net", "--allow-env", "src/main.ts"]
```

#### 3.1.3 æ•°æ®åº“é•œåƒ

```dockerfile
# database/Dockerfile
FROM postgres:14-alpine

ENV POSTGRES_DB=ai_evaluation
ENV POSTGRES_USER=ai_eval
ENV POSTGRES_PASSWORD=your_password

COPY init.sql /docker-entrypoint-initdb.d/

EXPOSE 5432
```

### 3.2 å®¹å™¨ç¼–æ’

#### 3.2.1 Docker Composeé…ç½®

```yaml
# docker-compose.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    ports:
      - "80:80"
    depends_on:
      - backend
    networks:
      - app-network

  backend:
    build: ./backend
    ports:
      - "3000:3000"
    environment:
      - DATABASE_URL=postgresql://ai_eval:password@postgres:5432/ai_evaluation
      - REDIS_URL=redis://redis:6379
      - ZHIJIYOYUN_API_URL=${ZHIJIYOYUN_API_URL}
      - ZHIJIYOYUN_API_KEY=${ZHIJIYOYUN_API_KEY}
    depends_on:
      - postgres
      - redis
    networks:
      - app-network

  postgres:
    image: postgres:14-alpine
    environment:
      - POSTGRES_DB=ai_evaluation
      - POSTGRES_USER=ai_eval
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init.sql:/docker-entrypoint-initdb.d/
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    volumes:
      - redis_data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:

networks:
  app-network:
    driver: bridge
```

#### 3.2.2 ç”Ÿäº§ç¯å¢ƒé…ç½®

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  frontend:
    build: ./frontend
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    networks:
      - app-network

  backend:
    build: ./backend
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    environment:
      - NODE_ENV=production
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  postgres:
    image: postgres:14-alpine
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - app-network

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.prod.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
      - nginx_logs:/var/log/nginx
    depends_on:
      - frontend
      - backend
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:
  nginx_logs:

networks:
  app-network:
    driver: overlay
    ipam:
      config:
        - subnet: 10.0.0.0/24
```

### 3.3 æœåŠ¡å‘ç°

#### 3.3.1 æœåŠ¡æ³¨å†Œæœºåˆ¶

```typescript
// æœåŠ¡æ³¨å†Œå™¨
class ServiceRegistry {
  private services = new Map<string, ServiceInfo>();

  register(service: ServiceInfo): void {
    this.services.set(service.name, {
      ...service,
      registeredAt: new Date(),
      lastHeartbeat: new Date()
    });
  }

  unregister(name: string): void {
    this.services.delete(name);
  }

  heartbeat(name: string): void {
    const service = this.services.get(name);
    if (service) {
      service.lastHeartbeat = new Date();
    }
  }

  getServices(): ServiceInfo[] {
    return Array.from(this.services.values());
  }

  getHealthyServices(): ServiceInfo[] {
    const now = new Date();
    const threshold = 30 * 1000; // 30ç§’

    return Array.from(this.services.values()).filter(
      service => now.getTime() - service.lastHeartbeat.getTime() < threshold
    );
  }
}

interface ServiceInfo {
  name: string;
  host: string;
  port: number;
  protocol: 'http' | 'https';
  health: 'healthy' | 'unhealthy';
  registeredAt: Date;
  lastHeartbeat: Date;
}
```

#### 3.3.2 æœåŠ¡å‘ç°é…ç½®

```yaml
# æœåŠ¡å‘ç°é…ç½®
service_discovery:
  enabled: true
  registry_url: "http://localhost:3001"
  heartbeat_interval: 30
  health_check:
    enabled: true
    interval: 10
    timeout: 5
    retries: 3

services:
  frontend:
    name: "frontend"
    host: "frontend"
    port: 80
    protocol: "http"
    health_check:
      path: "/health"
      expected_status: 200

  backend:
    name: "backend"
    host: "backend"
    port: 3000
    protocol: "http"
    health_check:
      path: "/api/health"
      expected_status: 200
```

### 3.4 è´Ÿè½½å‡è¡¡

#### 3.4.1 Nginxè´Ÿè½½å‡è¡¡é…ç½®

```nginx
# nginx/nginx.conf
upstream frontend_servers {
    least_conn;
    server frontend:80 max_fails=3 fail_timeout=30s;
}

upstream backend_servers {
    least_conn;
    server backend_1:3000 max_fails=3 fail_timeout=30s;
    server backend_2:3000 max_fails=3 fail_timeout=30s;
    server backend_3:3000 max_fails=3 fail_timeout=30s backup;
}

server {
    listen 80;
    server_name localhost;

    # å‰ç«¯è´Ÿè½½å‡è¡¡
    location / {
        proxy_pass http://frontend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # APIè´Ÿè½½å‡è¡¡
    location /api/ {
        proxy_pass http://backend_servers;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 30s;
        proxy_send_timeout 30s;
        proxy_read_timeout 30s;
    }
}
```

#### 3.4.2 å¥åº·æ£€æŸ¥é…ç½®

```nginx
# å¥åº·æ£€æŸ¥é…ç½®
upstream backend_servers {
    least_conn;
    server backend_1:3000 max_fails=3 fail_timeout=30s;
    server backend_2:3000 max_fails=3 fail_timeout=30s;
    server backend_3:3000 max_fails=3 fail_timeout=30s backup;
}

server {
    # å¥åº·æ£€æŸ¥ç«¯ç‚¹
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }

    # åç«¯å¥åº·æ£€æŸ¥
    location /backend/health {
        proxy_pass http://backend_servers/api/health;
        access_log off;
    }
}
```

---

## 4. æ•°æ®åº“éƒ¨ç½²

### 4.1 PostgreSQLéƒ¨ç½²

#### 4.1.1 ä¸»ä»å¤åˆ¶é…ç½®

```sql
-- ä¸»åº“é…ç½®
-- postgresql.conf
listen_addresses = '*'
wal_level = replica
archive_mode = on
archive_command = 'cp %p /var/lib/postgresql/archive/%f'
max_wal_senders = 3
wal_keep_segments = 64

-- pg_hba.conf
# å…è®¸ä»åº“è¿æ¥
host replication replicator 10.0.0.0/24 md5
```

#### 4.1.2 ä»åº“é…ç½®

```sql
-- ä»åº“é…ç½®
-- postgresql.conf
hot_standby = on
standby_mode = 'on'
primary_conninfo = 'host=10.0.4.10 port=5432 user=replicator'
restore_command = 'cp /var/lib/postgresql/archive/%f %p'
standby_mode = 'on'
```

#### 4.1.3 è¿æ¥æ± é…ç½®

```typescript
// æ•°æ®åº“è¿æ¥æ± é…ç½®
const dbConfig = {
  host: process.env.DB_HOST || 'localhost',
  port: parseInt(process.env.DB_PORT || '5432'),
  database: process.env.DB_NAME || 'ai_evaluation',
  username: process.env.DB_USER || 'postgres',
  password: process.env.DB_PASSWORD,
  pool: {
    min: 5,
    max: 20,
    acquireTimeoutMillis: 30000,
    createTimeoutMillis: 30000,
    destroyTimeoutMillis: 5000,
    idleTimeoutMillis: 30000,
    reapIntervalMillis: 1000,
    createRetryIntervalMillis: 200
  }
};
```

### 4.2 ç¼“å­˜éƒ¨ç½²

#### 4.2.1 Redisé›†ç¾¤é…ç½®

```yaml
# redis-cluster.conf
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
appendfilename "appendonly-7000.aof"
dbfilename "dump-7000.rdb"

# å†…å­˜é…ç½®
maxmemory 2gb
maxmemory-policy allkeys-lru
```

#### 4.2.2 Rediså“¨å…µé…ç½®

```yaml
# sentinel.conf
port 26379
sentinel monitor mymaster 10.0.4.20 6379 2
sentinel down-after-milliseconds 30000
sentinel parallel-syncs 1
sentinel failover-timeout 180000
sentinel auth-pass mymaster your_redis_password
```

### 4.3 æ–‡ä»¶å­˜å‚¨éƒ¨ç½²

#### 4.3.1 å¯¹è±¡å­˜å‚¨é…ç½®

```typescript
// æ–‡ä»¶å­˜å‚¨é…ç½®
const storageConfig = {
  provider: 's3', // å¯é€‰: s3, local, minio
  s3: {
    region: process.env.AWS_REGION,
    bucket: process.env.AWS_S3_BUCKET,
    accessKeyId: process.env.AWS_ACCESS_KEY_ID,
    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
    endpoint: process.env.AWS_S3_ENDPOINT // MinIOç«¯ç‚¹
  },
  local: {
    uploadPath: './uploads',
    maxFileSize: '10MB',
    allowedTypes: ['image/jpeg', 'image/png', 'application/pdf']
  }
};
```

### 4.4 æ•°æ®å¤‡ä»½ç­–ç•¥

#### 4.4.1 è‡ªåŠ¨å¤‡ä»½è„šæœ¬

```bash
#!/bin/bash
# backup.sh

# é…ç½®å˜é‡
BACKUP_DIR="/backups"
DB_HOST="localhost"
DB_PORT="5432"
DB_NAME="ai_evaluation"
DB_USER="postgres"
DB_PASSWORD="password"
RETENTION_DAYS=30

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ•°æ®åº“å¤‡ä»½
echo "å¼€å§‹æ•°æ®åº“å¤‡ä»½..."
pg_dump -h $DB_HOST -p $DB_PORT -U $DB_USER -d $DB_NAME > $BACKUP_DIR/db_backup_$(date +%Y%m%d_%H%M%S).sql

# æ–‡ä»¶å¤‡ä»½
echo "å¼€å§‹æ–‡ä»¶å¤‡ä»½..."
tar -czf $BACKUP_DIR/files_backup_$(date +%Y%m%d_%H%M%S).tar.gz /app/uploads

# æ¸…ç†æ—§å¤‡ä»½
echo "æ¸…ç†æ—§å¤‡ä»½..."
find $BACKUP_DIR -name "*.sql" -mtime +$RETENTION_DAYS -delete
find $BACKUP_DIR -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "å¤‡ä»½å®Œæˆ!"
```

#### 4.4.2 å¤‡ä»½è®¡åˆ’

```bash
# å¤‡ä»½è®¡åˆ’é…ç½®
# æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œå…¨é‡å¤‡ä»½
0 2 * * * /opt/scripts/backup.sh >> /var/log/backup.log 2>&1

# æ¯å‘¨æ—¥æ‰§è¡Œå¤‡ä»½éªŒè¯
0 3 * * 0 /opt/scripts/backup-verify.sh >> /var/log/backup.log 2>&1
```

---

## 5. CI/CDæµæ°´çº¿

### 5.1 æŒç»­é›†æˆ

#### 5.1.1 GitHub Actionsé…ç½®

```yaml
# .github/workflows/ci.yml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
        cache: 'npm'

    - name: Install dependencies
      run: |
        cd frontend && npm ci
        cd ../backend && npm ci

    - name: Run frontend tests
      run: |
        cd frontend
        npm run test:coverage
        npm run build

    - name: Run backend tests
      run: |
        cd backend
        npm run test:coverage

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/lcov.info

    - name: Build Docker images
      run: |
        docker build -t ai-evaluation-frontend:${{ github.sha }} ./frontend
        docker build -t ai-evaluation-backend:${{ github.sha }} ./backend

    - name: Run security scan
      run: |
        docker run --rm -v "$PWD":/app -w /app clairlocal/clair-scanner:latest scan
```

### 5.2 æŒç»­éƒ¨ç½²

#### 5.2.1 éƒ¨ç½²æµæ°´çº¿

```yaml
# .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Deploy to production
      run: |
        # æ›´æ–°ç”Ÿäº§ç¯å¢ƒé…ç½®
        sed -i "s/GITHUB_SHA/${{ github.sha }}/g" docker-compose.prod.yml

        # åœæ­¢æ—§æœåŠ¡
        docker-compose -f docker-compose.prod.yml down

        # æ‹‰å–æœ€æ–°é•œåƒ
        docker-compose -f docker-compose.prod.yml pull

        # å¯åŠ¨æ–°æœåŠ¡
        docker-compose -f docker-compose.prod.yml up -d

        # ç­‰å¾…æœåŠ¡å¯åŠ¨
        sleep 30

        # å¥åº·æ£€æŸ¥
        curl -f http://localhost/api/health

        # éªŒè¯éƒ¨ç½²
        ./scripts/verify-deployment.sh

    - name: Notify deployment
      if: success()
      run: |
        curl -X POST "${{ secrets.SLACK_WEBHOOK }}" \
          -H 'Content-type: application/json' \
          --data '{"text":"âœ… AIåŠ©è¯„ç³»ç»Ÿéƒ¨ç½²æˆåŠŸï¼ç‰ˆæœ¬: '"${{ github.sha }}"'"}'
```

#### 5.2.2 è“ç»¿éƒ¨ç½²é…ç½®

```yaml
# docker-compose.blue-green.yml
version: '3.8'

services:
  nginx-blue:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.blue.conf:/etc/nginx/nginx.conf
    depends_on:
      - frontend-blue
      - backend-blue

  nginx-green:
    image: nginx:alpine
    ports:
      - "8080:80"
    volumes:
      - ./nginx/nginx.green.conf:/etc/nginx/nginx.conf
    depends_on:
      - frontend-green
      - backend-green

  frontend-blue:
    build: ./frontend
    environment:
      - ENV_COLOR=blue

  frontend-green:
    build: ./frontend
    environment:
      - ENV_COLOR=green

  backend-blue:
    build: ./backend
    environment:
      - ENV_COLOR=blue

  backend-green:
    build: ./backend
    environment:
      - ENV_COLOR=green
```

### 5.3 è‡ªåŠ¨åŒ–æµ‹è¯•

#### 5.3.1 æµ‹è¯•è‡ªåŠ¨åŒ–é…ç½®

```bash
#!/bin/bash
# automated-tests.sh

echo "å¼€å§‹è‡ªåŠ¨åŒ–æµ‹è¯•..."

# 1. å•å…ƒæµ‹è¯•
echo "æ‰§è¡Œå•å…ƒæµ‹è¯•..."
npm run test:unit

# 2. é›†æˆæµ‹è¯•
echo "æ‰§è¡Œé›†æˆæµ‹è¯•..."
npm run test:integration

# 3. ç«¯åˆ°ç«¯æµ‹è¯•
echo "æ‰§è¡Œç«¯åˆ°ç«¯æµ‹è¯•..."
npm run test:e2e

# 4. æ€§èƒ½æµ‹è¯•
echo "æ‰§è¡Œæ€§èƒ½æµ‹è¯•..."
npm run test:performance

# 5. å®‰å…¨æµ‹è¯•
echo "æ‰§è¡Œå®‰å…¨æµ‹è¯•..."
npm run test:security

# 6. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
echo "ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š..."
npm run test:report

echo "è‡ªåŠ¨åŒ–æµ‹è¯•å®Œæˆï¼"
```

#### 5.3.2 æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

```typescript
// test-report-generator.ts
class TestReportGenerator {
  async generateReport(): Promise<TestReport> {
    const unitTestResults = await this.getUnitTestResults();
    const integrationTestResults = await this.getIntegrationTestResults();
    const e2eTestResults = await this.getE2ETestResults();
    const performanceTestResults = await this.getPerformanceTestResults();

    return {
      summary: {
        totalTests: unitTestResults.total + integrationTestResults.total +
                 e2eTestResults.total + performanceTestResults.total,
        passed: unitTestResults.passed + integrationTestResults.passed +
               e2eTestResults.passed + performanceTestResults.passed,
        failed: unitTestResults.failed + integrationTestResults.failed +
               e2eTestResults.failed + performanceTestResults.failed,
        passRate: this.calculatePassRate(
          unitTestResults, integrationTestResults, e2eTestResults, performanceTestResults
        )
      },
      details: {
        unit: unitTestResults,
        integration: integrationTestResults,
        e2e: e2eTestResults,
        performance: performanceTestResults
      },
      generatedAt: new Date()
    };
  }
}
```

### 5.4 å›æ»šæœºåˆ¶

#### 5.4.1 å›æ»šè„šæœ¬

```bash
#!/bin/bash
# rollback.sh

echo "å¼€å§‹å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬..."

# è·å–ä¸Šä¸€ä¸ªç‰ˆæœ¬çš„é•œåƒæ ‡ç­¾
PREVIOUS_VERSION=$(docker images --format "table {{.Repository}}:{{.Tag}}" | grep ai-evaluation | tail -n 2 | head -n 1 | awk '{print $2}')

echo "å›æ»šåˆ°ç‰ˆæœ¬: $PREVIOUS_VERSION"

# æ›´æ–°é…ç½®æ–‡ä»¶
sed -i "s/CURRENT_TAG/$PREVIOUS_VERSION/g" docker-compose.prod.yml

# åœæ­¢å½“å‰æœåŠ¡
docker-compose -f docker-compose.prod.yml down

# å¯åŠ¨ä¸Šä¸€ä¸ªç‰ˆæœ¬
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 60

# éªŒè¯å›æ»š
curl -f http://localhost/api/health

echo "å›æ»šå®Œæˆï¼"
```

#### 5.4.2 å›æ»šå†³ç­–è§„åˆ™

```typescript
// å›æ»šå†³ç­–è§„åˆ™
class RollbackDecision {
  shouldRollback(deploymentResult: DeploymentResult): boolean {
    const criticalThresholds = {
      healthCheckFailure: 0.1,      // å¥åº·æ£€æŸ¥å¤±è´¥ç‡
      errorRate5xx: 0.05,            // 5xxé”™è¯¯ç‡
      responseTimeP95: 5000,         // 95%å“åº”æ—¶é—´(ms)
      errorCount: 10                 // é”™è¯¯è®¡æ•°
    };

    return (
      deploymentResult.healthCheckFailure > criticalThresholds.healthCheckFailure ||
      deploymentResult.errorRate5xx > criticalThresholds.errorRate5xx ||
      deploymentResult.responseTimeP95 > criticalThresholds.responseTimeP95 ||
      deploymentResult.errorCount > criticalThresholds.errorCount
    );
  }
}
```

---

## 6. ç›‘æ§è¿ç»´

### 6.1 ç³»ç»Ÿç›‘æ§

#### 6.1.1 Prometheusé…ç½®

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'ai-evaluation-frontend'
    static_configs:
      - targets: ['frontend:3001']
    metrics_path: '/metrics'
    scrape_interval: 30s

  - job_name: 'ai-evaluation-backend'
    static_configs:
      - targets: ['backend:3000', 'backend-2:3000', 'backend-3:3000']
    metrics_path: '/metrics'
    scrape_interval: 15s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:9113']

alerting:
  alertmanagers:
    - static_configs:
      - targets:
        - alertmanager:9093
```

#### 6.1.2 å‘Šè­¦è§„åˆ™

```yaml
# alert_rules.yml
groups:
  - name: ai-evaluation.rules
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"

      - alert: DatabaseConnectionsHigh
        expr: pg_stat_activity_count{state="active"} > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connections high"
          description: "Active database connections: {{ $value }}"
```

### 6.2 æ—¥å¿—ç®¡ç†

#### 6.2.1 æ—¥å¿—é…ç½®

```typescript
// æ—¥å¿—é…ç½®
const loggerConfig = {
  level: process.env.LOG_LEVEL || 'info',
  format: 'json',
  transports: [
    new transports.Console({
      format: format.combine(
        format.colorize(),
        format.simple()
      )
    }),
    new transports.File({
      filename: 'logs/app.log',
      level: 'info',
      format: format.combine(
        format.timestamp(),
        format.json()
      )
    }),
    new transports.File({
      filename: 'logs/error.log',
      level: 'error',
      format: format.combine(
        format.timestamp(),
        format.json()
      )
    })
  ]
};
```

#### 6.2.2 ELK Stacké…ç½®

```yaml
# elasticsearch.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.10.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.10.0
    ports:
      - "5044:5044"
      - "5000:5000/tcp"
      - "9600:9600"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.10.0
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

### 6.3 å‘Šè­¦æœºåˆ¶

#### 6.3.1 AlertManageré…ç½®

```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'smtp.example.com:587'
  smtp_from: 'alerts@ai-evaluation.com'
  smtp_auth_username: 'alerts@ai-evaluation.com'
  smtp_auth_password: 'password'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://localhost:3001/alerts'
        send_resolved: true

  - name: 'slack.notifications'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts'
        send_resolved: true

  - name: 'email.notifications'
    email_configs:
      - to: 'admin@ai-evaluation.com'
        subject: '[AIåŠ©è¯„ç³»ç»Ÿ] å‘Šè­¦: {{ .GroupLabels.alertname }}'
        body: |
          å‘Šè­¦è¯¦æƒ…:
          å‘Šè­¦ç»„: {{ .GroupLabels.alertname }}
          çŠ¶æ€: {{ .Status }}
          å‘Šè­¦æ•°: {{ .Alerts | length }}
          è¯¦æƒ…: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}
```

#### 6.3.2 å‘Šè­¦é€šçŸ¥è„šæœ¬

```typescript
// å‘Šè­¦é€šçŸ¥æœåŠ¡
class AlertNotificationService {
  async sendAlert(alert: Alert): Promise<void> {
    try {
      // å‘é€Slacké€šçŸ¥
      await this.sendSlackNotification(alert);

      // å‘é€é‚®ä»¶é€šçŸ¥
      await this.sendEmailNotification(alert);

      // å‘é€é’‰é’‰é€šçŸ¥
      await this.sendDingTalkNotification(alert);

      console.log('å‘Šè­¦é€šçŸ¥å‘é€æˆåŠŸ');
    } catch (error) {
      console.error('å‘Šè­¦é€šçŸ¥å‘é€å¤±è´¥:', error);
    }
  }

  private async sendSlackNotification(alert: Alert): Promise<void> {
    const payload = {
      text: `ğŸš¨ ç³»ç»Ÿå‘Šè­¦`,
      attachments: [{
        color: this.getAlertColor(alert.severity),
        fields: [
          { title: 'å‘Šè­¦åç§°', value: alert.name, short: true },
          { title: 'ä¸¥é‡ç¨‹åº¦', value: alert.severity, short: true },
          { title: 'å‘Šè­¦æè¿°', value: alert.description, short: false },
          { title: 'å¼€å§‹æ—¶é—´', value: alert.startAt, short: true },
          { title: 'å½“å‰çŠ¶æ€', value: alert.status, short: true }
        ]
      }]
    };

    await fetch(process.env.SLACK_WEBHOOK_URL, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(payload)
    });
  }
}
```

### 6.4 è¿ç»´è‡ªåŠ¨åŒ–

#### 6.4.1 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# maintenance.sh

echo "å¼€å§‹è¿ç»´è‡ªåŠ¨åŒ–ä»»åŠ¡..."

# 1. æ¸…ç†æ—¥å¿—æ–‡ä»¶
echo "æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶..."
find /var/log -name "*.log" -mtime +7 -delete
find /app/logs -name "*.log" -mtime +7 -delete

# 2. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
echo "æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
find /tmp -name "tmp.*" -mtime +1 -delete

# 3. æ•°æ®åº“ç»´æŠ¤
echo "æ‰§è¡Œæ•°æ®åº“ç»´æŠ¤..."
psql -h localhost -U postgres -d ai_evaluation -c "VACUUM ANALYZE;"

# 4. ç³»ç»Ÿå¥åº·æ£€æŸ¥
echo "æ‰§è¡Œç³»ç»Ÿå¥åº·æ£€æŸ¥..."
./health-check.sh

# 5. å¤‡ä»½æ£€æŸ¥
echo "æ£€æŸ¥å¤‡ä»½å®Œæ•´æ€§..."
./backup-verify.sh

echo "è¿ç»´è‡ªåŠ¨åŒ–ä»»åŠ¡å®Œæˆï¼"
```

#### 6.4.2 å®šæ—¶ä»»åŠ¡é…ç½®

```bash
# crontab é…ç½®
# æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œå¤‡ä»½
0 2 * * * /opt/scripts/backup.sh

# æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ‰§è¡Œç³»ç»Ÿç»´æŠ¤
0 3 * * 0 /opt/scripts/maintenance.sh

# æ¯å°æ—¶æ‰§è¡Œå¥åº·æ£€æŸ¥
0 * * * * /opt/scripts/health-check.sh

# æ¯15åˆ†é’Ÿæ£€æŸ¥æœåŠ¡çŠ¶æ€
*/15 * * * * /opt/scripts/service-check.sh
```

---

## é™„å½•

### A. éƒ¨ç½²æ£€æŸ¥æ¸…å•

#### A.1 éƒ¨ç½²å‰æ£€æŸ¥

- [ ] ç¯å¢ƒé…ç½®æ–‡ä»¶å‡†å¤‡å®Œæ¯•
- [ ] æ•°æ®åº“å¤‡ä»½å®Œæˆ
- [ ] åŸŸåDNSé…ç½®å®Œæˆ
- [ ] SSLè¯ä¹¦é…ç½®å®Œæˆ
- [ ] ç›‘æ§ç³»ç»Ÿé…ç½®å®Œæˆ
- [ ] å‘Šè­¦é€šçŸ¥é…ç½®å®Œæˆ
- [ ] å›æ»šè®¡åˆ’åˆ¶å®šå®Œæˆ

#### A.2 éƒ¨ç½²åæ£€æŸ¥

- [ ] æ‰€æœ‰æœåŠ¡æ­£å¸¸å¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥å…¨éƒ¨é€šè¿‡
- [ ] æ•°æ®åº“è¿æ¥æ­£å¸¸
- [ ] ç¼“å­˜æœåŠ¡æ­£å¸¸
- [ ] æ—¥å¿—ç³»ç»Ÿæ­£å¸¸
- [ ] ç›‘æ§æ•°æ®æ­£å¸¸
- [ ] å‘Šè­¦ç³»ç»Ÿæ­£å¸¸
- [ ] ç”¨æˆ·è®¿é—®æ­£å¸¸

### B. æ•…éšœæ’æŸ¥æŒ‡å—

#### B.1 å¸¸è§é—®é¢˜æ’æŸ¥

| é—®é¢˜ç±»å‹ | æ£€æŸ¥æ­¥éª¤ | è§£å†³æ–¹æ³• |
|----------|----------|----------|
| æœåŠ¡å¯åŠ¨å¤±è´¥ | æ£€æŸ¥å®¹å™¨æ—¥å¿—ã€ç«¯å£å ç”¨ã€é…ç½®æ–‡ä»¶ | æŸ¥çœ‹æ—¥å¿—ï¼Œä¿®å¤é…ç½® |
| æ•°æ®åº“è¿æ¥å¤±è´¥ | æ£€æŸ¥ç½‘ç»œã€ç”¨æˆ·æƒé™ã€é˜²ç«å¢™ | è°ƒæ•´ç½‘ç»œå’Œæƒé™ |
| è´Ÿè½½å‡è¡¡å¼‚å¸¸ | æ£€æŸ¥åç«¯æœåŠ¡çŠ¶æ€ã€å¥åº·æ£€æŸ¥ | é‡å¯æœåŠ¡ï¼Œè°ƒæ•´é…ç½® |
| æ€§èƒ½é—®é¢˜ | æ£€æŸ¥èµ„æºä½¿ç”¨ã€æ…¢æŸ¥è¯¢ã€ç¼“å­˜ | ä¼˜åŒ–æŸ¥è¯¢ï¼Œæ‰©å®¹èµ„æº |

### C. éƒ¨ç½²å·¥å…·æ¸…å•

| å·¥å…·ç±»å‹ | å·¥å…·åç§° | ç”¨é€” | å®‰è£…æ–¹å¼ |
|----------|----------|------|----------|
| å®¹å™¨åŒ– | Docker | åº”ç”¨å®¹å™¨åŒ– | å®˜æ–¹äºŒè¿›åˆ¶ |
| ç¼–æ’ | Docker Compose | å®¹å™¨ç¼–æ’ | å®˜æ–¹äºŒè¿›åˆ¶ |
| ç›‘æ§ | Prometheus + Grafana | ç³»ç»Ÿç›‘æ§ | Dockeréƒ¨ç½² |
| æ—¥å¿— | ELK Stack | æ—¥å¿—ç®¡ç† | Dockeréƒ¨ç½² |
| CI/CD | GitHub Actions | æŒç»­é›†æˆéƒ¨ç½² | äº‘æœåŠ¡ |
| é…ç½®ç®¡ç† | Ansible | é…ç½®ç®¡ç† | åŒ…ç®¡ç†å™¨ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: V1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-23
**æœ€åæ›´æ–°**: 2025-11-23
**å®¡æ‰¹çŠ¶æ€**: å¾…å®¡æ‰¹
**ç»´æŠ¤è´£ä»»äºº**: è¿ç»´å·¥ç¨‹å¸ˆå›¢é˜Ÿ