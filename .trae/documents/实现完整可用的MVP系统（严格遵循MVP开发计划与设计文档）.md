## 范围与约束
- 严格遵循MVP开发计划与“02设计文档_MVP”所有规范与范围
- 职教云API暂不可用：采用可切换的模拟实现（Mock Provider），保证端到端闭环与数据口径一致
- AI分析引擎接入本地Ollama的Qwen3:8b模型（模型名与接口可通过配置切换），用于生成洞察与建议等自然语言内容；数值指标仍由数据库真实计算

## 后端实现（关键点）
- API版本与响应规范
  - 统一使用`/v1/*`前缀（保留`/api/*`兼容可选）；所有响应加入`code`与`timestamp`
- 职教云模拟实现（Mock Provider）
  - 新增`ZhijiaoyunProvider`接口与两种实现：`RealProvider`（现有服务）与`MockProvider`
  - `MockProvider`提供确定性、可分页的假数据（用户/课程/班级/出勤/作业/成绩/评价/活动/统计/同步），支持时间窗与过滤；数据分布匹配MVP可视化与分析需求
  - 在`config.zhijiaoyun.mode`切换`mock|real`；路由透明复用，不影响前端
  - 保留并修复`RealProvider`的签名异步BUG；为`MockProvider`补充分页与限流模拟
- 数据采集与清洗
  - 任务/日志/质量检查落库与分页；采集任务可基于`MockProvider`执行，质量报告对模拟数据给出规则校验结果
  - 清洗规则（去重、校验、转换）可应用于模拟数据，输出报告与落库
- AI分析引擎（规则+LLM）
  - 数值维度：替换随机值为真实查询（来自本地数据库与Mock采集落库数据）
  - LLM洞察：在`generateInsights`与`analyzeTrend`等位置引入`LLMAdapter`，当`config.ai.lmm.enabled=true`时，调用Ollama接口生成`strengths/weaknesses/recommendations/trends`自然语言内容
  - LLM调用：支持`/api/generate`或`/api/chat`两种模式，默认`model: qwen3:8b`，提示词模板可配置；错误回退到规则引擎的默认文本
- 评价指标计算
  - 结果持久化与分页查询完善；方法与聚合级别通过配置与代码路径真实计算
- 仪表盘与报表
  - 图表数据基于真实落库（包含Mock采集数据）；完善报表任务→生成文件→下载的完整链路

## 前端实现（关键点）
- 适配`/v1/*`与统一响应元信息；所有函数添加函数级注释
- 数据源开关：在“数据集成”页显示当前为Mock或Real；提供健康状态与同步入口
- 采集/清洗：任务管理、执行与日志、质量报告视图
- 分析与评价：教师分析（时间窗/规则选择）、批量任务与进度、结果详情；当LLM启用显示“AI生成洞察”标签
- 仪表盘与报表：图表与报表生成/下载完善

## 数据库与模型
- 扩充迁移：采集任务、清洗任务、分析任务、计算结果、规则存储、报表、活动日志等表
- 种子：预置用户、课程、教师、基础评价记录与适量模拟数据，以支持仪表盘与分析
- 索引：teacher_id、course_id、时间窗字段索引，保障≤5s响应

## 测试与质量
- 后端：单元（Provider/分析/计算）、集成（API路由）、性能（关键查询与任务）；覆盖率≥80%
- 前端：单元与集成（Vitest）、E2E（Playwright）覆盖核心业务路径
- LLM稳定性：对Ollama调用做超时与降级；记录提示词与输出摘要用于审计

## 部署与运维
- Docker Compose端到端：前端/后端/Postgres/Redis/Nginx；Ollama本地服务说明与健康探测
- 配置：`config.ai.llm`开关、`config.zhijiaoyun.mode`切换；生产环境密钥管理与日志/备份

## 验收与交付
- 业务闭环在Mock模式下完全可用：采集→清洗→分析/评价→可视化→报表下载
- LLM洞察在本地Ollama可用时启用；不可用时降级不影响数值分析
- 所有接口`/v1/*`可用且响应元信息统一；性能与测试覆盖满足文档要求

## 里程碑
- M1：版本化与响应统一、MockProvider落地、认证完善
- M2：任务/结果持久化闭环与真实数据计算替代随机值
- M3：仪表盘/报表、LLM洞察集成与测试性能达标
- M4：容器化部署、运维与上线

## 说明
- 开发阶段所有新增函数均添加函数级注释
- Mock数据的分布与边界将与文档示例和数据模型严格一致

请确认以上包含“职教云模拟实现”与“Ollama-Qwen3接入”的MVP实施计划，确认后我将开始分阶段实现与验证。